{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb3b5dcd",
   "metadata": {},
   "source": [
    "# LinkedIn Profile Scrapping Using BeautifulSoup & Selenium\n",
    "# By Eya Kaabachi "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df57dcff",
   "metadata": {},
   "source": [
    "We're going to cover a very popular topic about using Python language, automating web page crowling and extraction, which is Web Scrapping. Before we begin, here are some important rules to follow and understand:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d240c8",
   "metadata": {},
   "source": [
    "- Be respectful and ask for permission to scrape, don't bombard a website with requests or your IP address could get blocked!\n",
    "- Understand that websites and their code change regularly, which means that your code that works perfectly one day, could be totally inoperative the next.\n",
    "- Almost all interesting Web Scraping projects are unique and customized, you will have to make an effort to generalize the skills developed here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a961f5",
   "metadata": {},
   "source": [
    "Are you ready ? Ok let's start !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542ab56a",
   "metadata": {},
   "source": [
    "We should set a plan for our web scrapping method! We have 4 setps to go through to get the desired result:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81dc434",
   "metadata": {},
   "source": [
    "- **TASK 1** : Access the linkedin login page\n",
    "- **TASK 2** : Search for the profile we want\n",
    "- **TASK 3** : Navigate through next pages\n",
    "- **TASK 4** : Scrape the data of each profile and write the data to a .csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbc0b44",
   "metadata": {},
   "source": [
    "We're gonna need to install multiple libraries and modules :\n",
    "- Selenium : to interact with web browser\n",
    "- Chromedriver : for selenium to interact with\n",
    "- Beautifulsoup4: to extract the content of a website\n",
    "- Time: to delay an action by an amount of time\n",
    "- CSV: to work with csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b334197e",
   "metadata": {},
   "source": [
    "### Task 1:\n",
    "* **Task 1.1 :**\n",
    "- Open chrome browser\n",
    "- Access linkedin site\n",
    "* **Task 1.2 :**\n",
    "- key in the login credentials\n",
    "- login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e127b3",
   "metadata": {},
   "source": [
    "(PS : you need to create a .txt file and put your email in the first line and your password in the second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fabfc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84999c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c77bb073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 98.0.4758\n",
      "Get LATEST chromedriver version for 98.0.4758 google-chrome\n",
      "Trying to download new driver from https://chromedriver.storage.googleapis.com/98.0.4758.102/chromedriver_win32.zip\n",
      "Driver has been saved in cache [C:\\Users\\Eya Kaabachi\\.wdm\\drivers\\chromedriver\\win32\\98.0.4758.102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- finish initializing a driver\n",
      "-finish keying in email\n",
      "-finish keying in password\n",
      "-finish logging in\n"
     ]
    }
   ],
   "source": [
    "#step1 : login to linkedin\n",
    "\n",
    "#open chrome and login linkedin login site\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "url = 'https://www.linkedin.com/login'\n",
    "driver.get(url)\n",
    "print('- finish initializing a driver')\n",
    "sleep(2)\n",
    "\n",
    "# import username and password\n",
    "id = open('id.txt')\n",
    "line = id.readlines()\n",
    "username = line[0]\n",
    "password = line[1]\n",
    "\n",
    "#key in unsername\n",
    "email_field = driver.find_element_by_id ('username')\n",
    "email_field.send_keys(username)\n",
    "print('-finish keying in email')\n",
    "\n",
    "sleep(3)\n",
    "\n",
    "#key in password\n",
    "password_field = driver.find_element_by_name('session_password')\n",
    "password_field.send_keys(password)\n",
    "print('-finish keying in password')\n",
    "\n",
    "sleep(2)\n",
    "\n",
    "#click login button\n",
    "login_field = driver.find_element_by_xpath('//*[@id=\"organic-div\"]/form/div[3]/button')\n",
    "login_field.click()\n",
    "print('-finish logging in')\n",
    "sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89694d86",
   "metadata": {},
   "source": [
    "### Task 2:\n",
    "- Locate the search bar\n",
    "- input search keyword\n",
    "- search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb23bf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What profile do you want to scrape?ESPRIT\n",
      "-finish searching...\n"
     ]
    }
   ],
   "source": [
    "#step 2 : search for profile we want to crawl\n",
    "\n",
    "#locate the search bar element\n",
    "search_field = driver.find_element_by_xpath('//*[@id=\"global-nav-typeahead\"]/input')\n",
    "#input the search query to the search bar\n",
    "search_query = input('What profile do you want to scrape?')\n",
    "search_field.send_keys(search_query)\n",
    "#search \n",
    "search_field.send_keys(Keys.RETURN)\n",
    "print('-finish searching...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df72eea0",
   "metadata": {},
   "source": [
    "Also we have to locate the \"see all people results\" button and click it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd4b0e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to open \"see all people results\"\n",
    "sleep(2)\n",
    "login_field = driver.find_element_by_xpath('//*[@id=\"main\"]/div/div/div[2]/div[2]')\n",
    "login_field.click()\n",
    "\n",
    "#sleep(2)\n",
    "#login_field = driver.find_element_by_xpath('//*[@id=\"search-reusables__filters-bar\"]/div/div/button')\n",
    "#login_field.click()\n",
    "\n",
    "#sleep(2)\n",
    "#driver.execute_script('window.scrollTo(0, document.scrollHeight)')\n",
    "#login_field = driver.find_element_by_xpath('//*[@id=\"ember889\"]/ul/li[6]/fieldset/div/ul/li[1]')\n",
    "#login_field.click()\n",
    "\n",
    "#sleep(2)\n",
    "#driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')\n",
    "#sleep(2)\n",
    "#login_field = driver.find_element_by_xpath('//*[@id=\"ember571\"]')\n",
    "#login_field.click()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3744d6b3",
   "metadata": {},
   "source": [
    "### Task 3:\n",
    "* **Task 3.1 :**\n",
    "- Put all the urls of one page in a list\n",
    "* **Task 3.2 :**\n",
    "- Crawl all the pages and put the urls in one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "422cea52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.linkedin.com/search/results/people/', 'https://www.linkedin.com/in/ncib-lotfi-68443963', 'https://www.linkedin.com/in/amri-omar', 'https://www.linkedin.com/in/guesmi-mohamed-208309202']\n"
     ]
    }
   ],
   "source": [
    "#step 3 : scrape the urls of the profiles\n",
    "def GetURL():\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    profiles = soup.find_all('a', {'class':\"app-aware-link\",'target':''} )\n",
    "    all_profile_url = []\n",
    "    for profile in profiles:\n",
    "        profile_id = profile.get('href')\n",
    "        profile_url = profile_id.rsplit('?',1)[0]\n",
    "        if profile_url not in all_profile_url:\n",
    "            all_profile_url.append(profile_url)\n",
    "    return all_profile_url\n",
    "print(GetURL())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "addf3e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many pages you want to scrape: 1\n"
     ]
    }
   ],
   "source": [
    "    input_page = int(input('How many pages you want to scrape: '))\n",
    "    urls_all_page = []\n",
    "    for page in range(input_page):\n",
    "            url_one_page = GetURL()\n",
    "            sleep(2)\n",
    "            driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "            sleep(2)\n",
    "            next_button = driver.find_element_by_class_name('artdeco-pagination__button--next')\n",
    "            next_button.click()\n",
    "            urls_all_page =urls_all_page + url_one_page\n",
    "            sleep(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c565d36",
   "metadata": {},
   "source": [
    "### Task 4:\n",
    "- extract data about one profile\n",
    "- go through all the list of urls\n",
    "- put the data into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53128659",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 4 : scrape the data of 1 linkedin profile, and write the data to a .csv file\n",
    "with open('output.csv','w', newline = '') as file_output:\n",
    "    headers = ['Name','Job Title', 'Location', 'URL']\n",
    "    writer = csv.DictWriter(file_output, delimiter =',', lineterminator='\\n', fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    for linkedin_URL in urls_all_page:\n",
    "        driver.get(linkedin_URL)\n",
    "        sleep(2)\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        name_div = soup.find('div',{'class':'mt2 relative'})\n",
    "        name_loc = name_div.find_all('div')\n",
    "        name = name_loc[0].find('h1').get_text().strip()\n",
    "        print('- profile name is : ',name)\n",
    "        loc = name_loc[5].find('span')\n",
    "        location=loc.get_text().strip()\n",
    "        print('- profile location is : ',location)\n",
    "        profile_title = name_loc[2].get_text().strip()\n",
    "        print('- profile title is : ', profile_title)\n",
    "        print('\\n')\n",
    "        writer.writerow({headers[0]:name, headers[1]:location, headers[2]:profile_title, headers[3]:linkedin_URL})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d13777",
   "metadata": {},
   "source": [
    "And there you have it! you can customise the data you want to extract as you prefer. You are welcome :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
